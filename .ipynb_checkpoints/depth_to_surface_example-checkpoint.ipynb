{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the surface area calculator on Tantangara Reservoir\n",
    "This notebook is a proof of concept that converts depth to surface area for Tantangara reservoir. You can try running another reservoir if you like, just change the file name and you'll probably have to change the buffer too for the satellite query. This notebook does not use the bounding boxes so getting the satellite image right is a bit hard. This was the first notebook I made before I fixed up the queries and stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant python modules and data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import rasterio.crs\n",
    "from pandas import DataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import datacube\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Scripts')\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from dea_datahandling import wofs_fuser #this joins wofs data across tiles correctly\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.utils import masking\n",
    "from datacube.helpers import ga_pq_fuser, write_geotiff\n",
    "#from digitalearthau.utils import wofs_fuser\n",
    "#import DEAPlotting, DEADataHandling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter name of the depth gauge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User inputs\n",
    "csv = '\n",
    "reservoirs_shape_file = '00_Library_reservois/00_Library_reservois.shp'\n",
    "depth_interval_limit = 25\n",
    "todays_date = '14-04-2021'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(reservoirs_shape_file)\n",
    "\n",
    "#Get the name and ID of the gauge \n",
    "gauge_number_df = pd.read_csv(csv, nrows=1, escapechar='#')\n",
    "column2 = list(gauge_number_df)[1]\n",
    "gauge_number_df = gauge_number_df.rename(columns = {column2 : 'gauge_ID'})\n",
    "ID = gauge_number_df.at[0, 'gauge_ID']\n",
    "ID_str = ID.astype(str)\n",
    "\n",
    "query = {'time': ('01-01-1988', todays_date)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "query.update({'geopolygon': geom})\n",
    "\n",
    "wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, \n",
    "                     group_by='solar_day', fuse_func = wofs_fuser, **query) #wofs_fuser is important, it fixes thing on the edge of tiles\n",
    "\n",
    "poly_mask = xr_rasterize(gdf.iloc[[index]], wofs_albers)\n",
    "wofs_albers = wofs_albers.where(poly_mask, other=wofs_albers.water.nodata) #put other argument or all the data turns into 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise data by depth (instead of time) and split into 1m intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make depth duration curve\n",
    "gauge_data = pd.read_csv(csv,\n",
    "                error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                         parse_dates=['Timestamp'], \n",
    "                         index_col=('Timestamp'),\n",
    "                        date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0])) #Robbi wrote this line\n",
    "gauge_data = gauge_data.dropna()\n",
    "gauge_data = gauge_data.sort_values('Value')\n",
    "gauge_data['rownumber'] = np.arange(len(gauge_data))\n",
    "gauge_data['Exceedence'] = (1-(gauge_data.rownumber/len(gauge_data)))*100\n",
    "gauge_data = gauge_data.drop(columns=['Interpolation Type', 'Quality Code'])\n",
    "\n",
    "#Get the depth range and intervals\n",
    "depth_integers = gauge_data.astype(np.int64)\n",
    "max_depth = depth_integers.Value.max()\n",
    "min_depth = depth_integers.Value.min()\n",
    "integer_array = depth_integers.Value.unique()\n",
    "integer_list = integer_array.tolist()\n",
    "\n",
    "print('there are this many 1 meter intervals for depth:', len(integer_list))\n",
    "gauge_data.plot(x='Exceedence', y='Value', figsize=(11,7))\n",
    "plt.ylabel('depth (m above sea level)')\n",
    "plt.grid(True)\n",
    "plt.title('Depth duration curve: how much % of time the depth was at least this high.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load satellite data parameters\n",
    "This is how to query satellite data from the Geoscience Datacube. The dc.load() function loads the query, but we'll pass a Dask argument which will prevent it from loading actual images, we just want the parameters (this is way faster). At the moment I'm using lat and lon with a buffer to query the satellite data, but I want to change this to using a bounding box, which I made in ArcGIS. That would get the query more exact for every single reservoir. But I don't know how to write a query using a bounding box geoDataFrame, I'll figure it out later I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dask loading wofs_albers data\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),    \n",
    "         'time': ('1980-01-01', '2019-08-22'), \n",
    "         'crs': 'EPSG:3577'} \n",
    "dc = datacube.Datacube(app='dc-WOfS')\n",
    "\n",
    "#use the Geoscience Datacube function .load() to load the satellite data\n",
    "#use dask_chunks argument to load parameters only, not images (which would take hours!)\n",
    "wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, group_by='solar_day', **query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link satellite data with the stream gauge data\n",
    "use Xarray to merge gauge data with satellite data on the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_data_xr = gauge_data.to_xarray() #convert gauge data to xarray\n",
    "merged_data = gauge_data_xr.interp(Timestamp=wofs_albers.time) #use xarrays .interp() function to merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop load satellite data for each 1m of depth\n",
    "This one takes a while because of loading images, the cloudmask and the plotting. It would be nice if this could be faster. Technically the images don't need to be plotted, but I thought for the example notebook people would want to see the pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_area_list = []\n",
    "\n",
    "for i in integer_list:\n",
    "    specified_level = merged_data.where((merged_data.Value > i) & \n",
    "                                    (merged_data.Value < i+1), drop=True)\n",
    "    date_list = specified_level.time.values\n",
    "    specified_passes = wofs_albers.sel(time=date_list).compute() #This .compute() Xarray function loads actual images\n",
    "    #cloudmask (Claire Krause wrote this for me)\n",
    "    cc = masking.make_mask(specified_passes.water, cloud=True)\n",
    "    ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "    # Calculate the total number of pixels per timestep\n",
    "    npixels_per_slice = (specified_passes.water.shape[1] * \n",
    "                         specified_passes.water.shape[2])\n",
    "    cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "    clear_specified_passes = specified_passes.water.isel(\n",
    "        time=cloud_pixels_fraction < 0.2)\n",
    "    wet = masking.make_mask(clear_specified_passes, wet=True).sum(dim='time')\n",
    "    dry = masking.make_mask(clear_specified_passes, dry=True).sum(dim='time')\n",
    "    clear = wet + dry\n",
    "    frequency = wet / clear\n",
    "    frequency = frequency.fillna(0)  \n",
    "   \n",
    "    #Get area from the satellite data\n",
    "    #get the frequency array\n",
    "    frequency_array = frequency.values\n",
    "    #Turn any pixel in the frequency array with a value greater than 0.2 into a pixel of value 1\n",
    "    #if the pixel value is 0.2 or lower it gets value 0\n",
    "    is_water = np.where((frequency_array > 0.2),1,0)\n",
    "    #give the 'frequency' xarray back its new values of zero and one\n",
    "    frequency.values = is_water\n",
    "    #sum up the pixels\n",
    "    number_water_pixels = frequency.sum(dim=['x', 'y'])\n",
    "    #get the number\n",
    "    number_water_pixels = number_water_pixels.values.tolist()\n",
    "    #multiply by pixel size to get area in m2\n",
    "    area_m2 = number_water_pixels*(25*25)\n",
    "    \n",
    "    surface_area_list.append(area_m2)\n",
    "    print('This is the area as calculated from wet pixels at', i, 'meters', area_m2)\n",
    "\n",
    "    #Plotting the image\n",
    "    frequency.plot(figsize = (7,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix zero values\n",
    "Eventually I will figure out a way to fix all zero or bad values for any reservoir but I'm just gonna hard code this in for now because I got other things I need to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_surface_area_list = [i if i > 0 else 12446875 for i in surface_area_list] \n",
    "new_surface_area_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrograph to monthly surface area\n",
    "The hydrograph is depth over time. Now we have surface area for depth, so we can get surface area over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_hydrograph = pd.read_csv(csv,\n",
    "                error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                         parse_dates=['Timestamp'], #Tells it this column is date format\n",
    "                         index_col=('Timestamp'), #Tells it to set Timestamp as the index column\n",
    "                        date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0])) #turns timestamp into date\n",
    "orig_hydrograph = orig_hydrograph.drop(columns=['Interpolation Type', 'Quality Code'])\n",
    "orig_hydrograph.plot(figsize=(16, 5))\n",
    "print('This is the original hydrograph: depth over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of depth to surface area \n",
    "depth_to_area_df = DataFrame(integer_list, columns=['Depth'])\n",
    "depth_to_area_df['Surface Area'] = new_surface_area_list\n",
    "depth_to_area_df['Name'] = name\n",
    "depth_to_area_df['ID'] = ID_str\n",
    "depth_to_area_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for merge\n",
    "First we gotta append the depth integers to the original hydrograph (as opposed to the depth values). This way we can match it up to the surface area using the table we created above (we will be merging the table with the hydrograph on the depth integer column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_hydrograph = orig_hydrograph.dropna()\n",
    "\n",
    "hydrograph1 = pd.read_csv(csv,\n",
    "                error_bad_lines = False, skiprows=9, escapechar='#', parse_dates=['Timestamp'])\n",
    "hydrograph1 = hydrograph1.drop(columns=['Interpolation Type', 'Quality Code'])\n",
    "hydrograph1 = hydrograph1.dropna()\n",
    "\n",
    "depth_integers = hydrograph1.Value.astype(np.int64)\n",
    "depth_integers_list = depth_integers.to_list()\n",
    "\n",
    "orig_hydrograph['Depth'] = depth_integers_list\n",
    "orig_hydrograph['Date'] = orig_hydrograph.index\n",
    "orig_hydrograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert depth to surface area and make a table\n",
    "We can now add surface area to the original dataframe, drop the depth column and plot the surface area over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = (orig_hydrograph\n",
    "          .merge(depth_to_area_df[['Surface Area', 'Depth', 'Name', 'ID']], on='Depth'))\n",
    "df1 = merged.sort_values(['Date'])\n",
    "#pd.read_csv('surface_area_timeseries.csv', parse_dates=['Date'])\n",
    "df = df1.set_index(['Date'])\n",
    "#Use this pandas function MS (monthly summary) to \n",
    "df = df.resample('MS').sum()\n",
    "df = df.drop(columns = ['Value', 'Depth'])\n",
    "df['Name'] = df1.at[0, 'Name']\n",
    "df['ID'] = df1.at[0, 'ID']\n",
    "df.plot(figsize=(16, 5))\n",
    "print('This is a table and the graph of surface area over time (monthly). You can save this out as a csv file')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nice one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
