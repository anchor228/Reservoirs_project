{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate depth to surface area tables by merging satellite data with depth gauge data\n",
    "This notebook merges gauge data with satellite data for multiple reservoirs at a time (in the 00_Library directory). It outputs a depth to surface area table for the reservoirs. The inputs are 1) a shapefile of reservoirs with their gauge ID attached and 2) the folder containing the csv files of gauge data. At the top of the screen is a tab called Kernel. If you want to start the notebook again at anytime and clear everything, you can select Restart Kernel and Clear All Outputs. The available memory is at the bottom of the screen. You have 15GB of memory and if the memory is exceeded the kernel will crash (but it's ok you can just start it again). Read the instructions carefully to run the code blocks and understand what's going on at each step. Press Shift + Enter to run a code block. Good Luck! - Katey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "Here's all the standard python modules you're gonna need. There's some special DEA modules in here to help handle the satellite data. You might get some warnings about depreciation or whatever but just ignore them. Those square brackets at the top left of the code cell will make a star while it's running and then a number when it's finished running. Loading the modules should only take a second to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/geopandas/_compat.py:88: UserWarning: The Shapely GEOS version (3.7.2-CAPI-1.11.0 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.0-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import rasterio.crs\n",
    "from tqdm.auto import tqdm #this one is a loading bar, it's cool to add loading bars to loops\n",
    "from pandas import DataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import datacube\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../Scripts')\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from dea_datahandling import wofs_fuser #this joins wofs data across tiles correctly\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.utils import masking\n",
    "from datacube.helpers import ga_pq_fuser, write_geotiff\n",
    "#from digitalearthau.utils import wofs_fuser\n",
    "#import DEAPlotting, DEADataHandling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs\n",
    "The inputs are the reservoir shapefile with the gauge IDs attached and the directory that contains the csv files of gauge data. We can also define how deep the reservoir should be before we start taking every 2m instead of every 1m. It's better for larger reservoirs to take every 2m because in my opinion it's more accurate to do this because it improves the image quality of each depth slice. I recommend 25 or 30 as the limit. Also put todays date in 'dd-mm-yyyy' format to get the latest satellite data. The image_cap variable is a little hard to explain, but basically this is how many images you are willing to load per depth interval. The more images you load, the more accurate the results will be but the longer it will take to run this script. If you just want to quickly see how this script works, set the image cap to 10 (approx. 20 minutes to calculate the surface areas of 150 reservoirs). If you want the most accurate results you don't want to limit the images at all so set the image cap to 10000 to effectively unlimit it (approx. 3 hours, could maybe crash).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User inputs\n",
    "reservoirs_shape_file = 'Named_Reservoirs/Named_Reservoirs5.shp'\n",
    "directory = 'Library' # this is where your csv files of gauge data are\n",
    "depth_interval_limit = 25\n",
    "todays_date = '11-05-2021'\n",
    "image_cap = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and Dask load the satellite data for all the reservoirs\n",
    "'Dask load' which is the dask_chunks = { } argument in the dc.load line means you only load satellite data parameters, not the actual images, which is good for linking the satellite data to the gauge data without it taking literally hours. So basically, in this box we read the shapefile with the reservoir polygons and query the satellite data with the polygons. Most of the query is done in a loop because we have to make hundreds of queries, one for each reservoir. To handle all these queries, we put them in a dictionary and use the gauge ID as the key for each query, so we can call the satellite data later on by the gauge ID. This is great, because the csv files of the depth gauge data all have the gauge ID in them so it's going to be easy to match up the satellite data of each reservoir with the gauge data just by using the gauge ID as the handle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dfa43894e047ef959d6fd60238d6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/426 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf = gpd.read_file(reservoirs_shape_file)\n",
    "\n",
    "query = {'time': ('01-01-1988', todays_date)} \n",
    "         #'crs': 'EPSG:3577'}\n",
    "dc = datacube.Datacube(app='dc-WOfS') #this is how you access the open data cube where the satellite data is\n",
    "\n",
    "results = {} \n",
    "\n",
    "#tqdm is gonna make the bar. tqdm is Arabic abbreviation for 'progress'\n",
    "for index, row in tqdm(gdf.iterrows(), total=len(gdf)):\n",
    "    geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "    query.update({'geopolygon': geom})\n",
    "    \n",
    "    wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, \n",
    "                         group_by='solar_day', fuse_func = wofs_fuser, **query) #wofs_fuser is important, it fixes thing on the edge of tiles\n",
    "    \n",
    "    poly_mask = xr_rasterize(gdf.iloc[[index]], wofs_albers)\n",
    "    wofs_albers = wofs_albers.where(poly_mask, other=wofs_albers.water.nodata) #put other argument or all the data turns into 0\n",
    "    \n",
    "    results.update({str(row['gauge_ID']): wofs_albers}) #The handle for dictionary objects is the gauge ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop read all the csv files in 00_Library\n",
    "In the previous code block we made a dictionary of the wofs data with the gauge ID as the key. We now need a dictionary of the depth data with, again, the gauge ID as the key. Then we can match them up later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe8478816704b17b77bf751fc3c4fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make a list of the file names so we can call them with pandas\n",
    "file_list = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_list.append(os.path.join(directory, filename))\n",
    "\n",
    "#Read the gauge files twice, once to get ID and second to get the data. Append them together in a dictionary\n",
    "#May as well make a list of IDs here because we will use it later\n",
    "data_dict = {}        \n",
    "ID_list = []\n",
    "#let's use tqdm again to make a progress bar. The bar is so cool I love this module\n",
    "for i in tqdm(file_list, total=len(file_list)):\n",
    "    df = pd.read_csv(i, nrows=1, escapechar='#')\n",
    "    column = df.iloc[:,[1]] #This is the column with the ID in it\n",
    "    ID = list(column)\n",
    "    ID = ID[0]\n",
    "    ID = df.at[0, ID]\n",
    "    ID_list.append(str(ID))\n",
    "    #now read again this time to get the actual data\n",
    "    data = pd.read_csv(i, error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                         parse_dates=['Timestamp'], \n",
    "                         index_col=('Timestamp'),\n",
    "                        date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0]))\n",
    "    data = data.drop(columns=['Quality Code', 'Interpolation Type'])\n",
    "    data_dict.update({str(ID): data}) #Now we have the gauge data, again with the gauge ID as the handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a function that generates the depth slices of a reservoir and gets the depth to surface area relationship\n",
    "This is my first time writing a function, Matthew and Bex from DEA helped me write it. It's a function that you can apply to one reservoir to load all the passes that have been over that reservoir since 1988, cloud mask them, organise them into depth intervals, stack the images for each depth interval on top of each other to make one average image for each depth and then count the pixels that have water in them to get the surface area at each depth. Then after we define this function we can loop it over all of the reservoirs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prod(ID_caller, gauge_data, wofs_albers, make_plots = False) -> 'depth slices': \n",
    "    \"\"\"\n",
    "    This function takes the gauge data and the wofs data,\n",
    "    cloud masks the images and counts the pixels in each depth slice.\n",
    "    It returns a list of all the surface areas per depth.\n",
    "    \n",
    "    \"\"\"\n",
    "    #Get the depth range and intervals\n",
    "    gauge_data = gauge_data.dropna()\n",
    "    depth_integers = gauge_data.astype(np.int64)\n",
    "    max_depth = depth_integers.Value.max()\n",
    "    min_depth = depth_integers.Value.min()\n",
    "    integer_array = depth_integers.Value.unique()\n",
    "    integer_list = integer_array.tolist()\n",
    "    \n",
    "        #Take every 2m of depth if the reservoir is large \n",
    "    if len(integer_list) > 25:\n",
    "        integer_list2 = integer_list[::2]\n",
    "        #print('This reservoir will take every 2m instead of every 1m, because it is quite large.')\n",
    "    else:\n",
    "            integer_list2 = integer_list\n",
    "            #print('This reservoir will take every 1m of depth')\n",
    "    \n",
    "    gauge_data_xr = gauge_data.to_xarray() #convert gauge data to xarray\n",
    "    merged_data = gauge_data_xr.interp(Timestamp=wofs_albers.time) #use xarrays .interp() function to merge\n",
    "\n",
    "    surface_area_list = []\n",
    "\n",
    "    for i in tqdm(integer_list2, leave = False):\n",
    "        if len(integer_list) > depth_interval_limit: #here's where the depth interval limit you set is taken into account\n",
    "            specified_level = merged_data.where((merged_data.Value > i) & \n",
    "                                (merged_data.Value < i+2), drop=True)\n",
    "        else:\n",
    "            specified_level = merged_data.where((merged_data.Value > i) & \n",
    "                                (merged_data.Value < i+1), drop=True)\n",
    "\n",
    "\n",
    "        date_list = specified_level.time.values[:image_cap] #caps images at x per slice \n",
    "        n_images_used = int(len(date_list))\n",
    "        specified_passes = wofs_albers.sel(time=date_list).compute() #This .compute() Xarray function loads actual images\n",
    "        #cloudmask (Claire Krause wrote this for me)\n",
    "        cc = masking.make_mask(specified_passes.water, cloud=True)\n",
    "        ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "        # Calculate the total number of pixels per timestep\n",
    "        npixels_per_slice = (specified_passes.water.shape[1] * \n",
    "                             specified_passes.water.shape[2])\n",
    "        cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "        clear_specified_passes = specified_passes.water.isel(\n",
    "            time=cloud_pixels_fraction < 0.2) #has to be under 20% cloudy to pass\n",
    "        wet = masking.make_mask(clear_specified_passes, wet=True).sum(dim='time')\n",
    "        dry = masking.make_mask(clear_specified_passes, dry=True).sum(dim='time')\n",
    "        clear = wet + dry\n",
    "        frequency = wet / clear\n",
    "        frequency = frequency.fillna(0)  \n",
    "\n",
    "        #Get area from the satellite data\n",
    "        #get the frequency array\n",
    "        frequency_array = frequency.values\n",
    "        n_images_after_masking = len(clear_specified_passes.time)\n",
    "        #Turn any pixel in the frequency array with a value greater than 0.2 into a pixel of value 1\n",
    "        #if the pixel value is 0.2 or lower it gets value 0\n",
    "        is_water = np.where((frequency_array > 0.2),1,0) #has to be water in more than 20% of images to count\n",
    "        #give the 'frequency' xarray back its new values of zero and one\n",
    "        frequency.values = is_water\n",
    "        #sum up the pixels\n",
    "        number_water_pixels = frequency.sum(dim=['x', 'y'])\n",
    "        #get the number\n",
    "        number_water_pixels = number_water_pixels.values.tolist()\n",
    "        #multiply by pixel size to get area in m2\n",
    "        area_m2 = number_water_pixels*(25*25)\n",
    "\n",
    "        surface_area_list.append([ID_caller, i, area_m2, n_images_used, n_images_after_masking])\n",
    "        #print('This is the area as calculated from wet pixels at', i, 'meters', area_m2)\n",
    "\n",
    "        #Plotting the image\n",
    "        if make_plots:\n",
    "            frequency.plot(figsize = (7,5))\n",
    "    del wofs_albers\n",
    "    del specified_passes\n",
    "    del cc\n",
    "    del clear_specified_passes\n",
    "    del wet\n",
    "    del dry\n",
    "    del clear\n",
    "    del frequency\n",
    "    #delete the images when you finish each reservoir (otherwise the memory will run out and the kernel will break)\n",
    "    return surface_area_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the function for all of the reservoirs\n",
    "OK this is the part that's going to take a while. How long this part takes depends on if you capped the images or not. Now we will loop over all of the gauge data and apply the function we just made. If you're doing this with the image cap at 10000, make sure you shutdown any other notebooks to free up memory (see the icon on the left with the circle and square to shutdown other notebooks). It's going to tell you it didn't find some of the gauges because not all of the gauges are matched up to a reservoir in the reservoirs shapefile (because of issues I'm having with the spatial join of the gauge points to the reservoir polygons, need to fix later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3613868b1f0643cea4814bedddcac231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  179.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  TAYLORS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  416030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  sp-o10814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  136024B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  139.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  421078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  401569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  229406A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  DAM_MANGRV.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  M316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  210097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  130216A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  sp-o11454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  219027\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  229607A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  212212\n",
      "we didnt find 212212\n",
      "Working on gauge  sp-o10438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  412107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  125015A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on gauge  401027\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38abc66797444186876b7c7e38450751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "array_list = []\n",
    "\n",
    "\n",
    "def listsplit(N, K=1):\n",
    "    length = len(N)\n",
    "    return [N[i*length/K: (i+1)*length/K] for i in range(K)]\n",
    "\n",
    "\n",
    "for ID in tqdm(ID_list, total=len(ID_list)):\n",
    "    print(\"Working on gauge \", ID)\n",
    "    if (ID in data_dict.keys()) and (ID in results.keys()):\n",
    "        data = image_prod(ID, data_dict[ID], results[ID], make_plots = False)\n",
    "        array_list.append(data)\n",
    "        \n",
    "        del data\n",
    "    else:\n",
    "        print('we didnt find', ID)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a look up table of depth to surface area for each gauge ID\n",
    "Nice, now we can make the output, which is a table. The columns will be the gauge ID, the depth interval (AHD), the corresponding surface area (m2) and the number of satellite images each surface area was calculated from. The number of images column is useful because it can indicate how accurate the surface area calculation was. If you only get 3 images to calculate the surface area with, it won't be as accurate as a surface area calculated from 100 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up_table = []\n",
    "for i in array_list:\n",
    "    df = DataFrame(i, columns = ['ID', 'Depth', 'Surface Area', 'number of images before masking', 'number of images after masking'])\n",
    "    look_up_table.append(df)\n",
    "\n",
    "df = pd.concat(look_up_table)\n",
    "df = df.set_index(\"ID\")  \n",
    "df = df.drop(columns=['number of images before masking'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the look up table as a csv file\n",
    "Well done! This is the initial depth to surface area table. When you run this next cell, the table above will appear as a csv file in your sandbox files. Then you can right click it and download to your local computer and open it in excel. This table will also be the input for the next step, which is to correct for bad quality images and also fill in every other meter for those big reservoirs we took every 2m for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('depth_to_surface_polygon_drill.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
