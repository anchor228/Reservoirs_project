{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make final depth to surface tables\n",
    "This notebook generates the final depth to surface area tables for one reservoir at a time. You have to do one at a time, because each reservoir is different and you have to make a decision on each one about which images to use. You should have generated image profiles for each reservoir by now, so you can see the actual images from which the surface area is being calculated. This notebook is a little bit hard to use, so read the markdowns carefully.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/geopandas/_compat.py:88: UserWarning: The Shapely GEOS version (3.7.2-CAPI-1.11.0 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.0-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "#import glob    #This one lets you read all the csv files in a directory\n",
    "import rasterio.crs\n",
    "from tqdm.auto import tqdm #this one is a loading bar, it's cool to add loading bars to loops\n",
    "from pandas import DataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import datacube\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../Scripts')\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from dea_datahandling import wofs_fuser\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.utils import masking\n",
    "from datacube.helpers import ga_pq_fuser, write_geotiff\n",
    "#from digitalearthau.utils import wofs_fuser\n",
    "#import DEAPlotting, DEADataHandling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv and split into depth intervals\n",
    "This is where you enter the location of the csv file of the initial depth to surface area table you made. The bad_rows variable can be used if you want to skip the lowest depths, for example if the first image is empty, eg surface area = 0, you will want to skip that, so put bad_rows = [0]. There is an argument in the code that says 'if the surface area for x depth is lower than x-1 depth, delete this image', because surface area going down with increasing depth obviously doesn't make sense. So if the second image is also 0, you don't have to enter it as a bad_row, because it's going to be deleted anyway. If you have an image at say, depth 6 that is going to be counted because it is a larger surface area than depth 5, however you notice that the image quality is bad, you can enter it as a bad_row so it doesn't get counted, so you might enter bad_rows = [0, 6]. If you have 0 or bad images for higher depths, don't worry because they are getting skipped anyway so you don't have to enter them. If you don't have any images that need to manually be skipped, hash this line out. The bad_row variable is just extra making sure you don't use bad images to calculate surface area. \n",
    "\n",
    "There is also an argument in this code block that says 'if there are more than 25 depth intervals, only use every 2m instead of every 1m'. This will give you better quality images because each image will be made of more passes. For big reservoirs like Blowering which varies by 70 meters, I think it's more accurate to have good quality images for every second meter and interpolate every other depth rather than try to make 70 images with only a few passes in each. But make a decision your self, if you have a reservoir that varies by 30m but you think the images are really good, change the code so it won't take every 2m and keep using every 1m.\n",
    "\n",
    "There's also some annoying things about this box that I haven't really bothered to try and fix. Some of the gauge IDs are pure numbers, some of them have letters, making them strings. The code has to read the ID as a string, so if the gauge ID is a number, you will have to convert it to a string. If it's already a string and you try to convert it to a string, it will throw an error. I just hash the 'ID_str = ID_str.astype(str)' line in and out as needed. The other thing is that sometimes 2 bounding boxes will be called, which will make you get an error later (there might be 2 boxes because the spatial join matched one gauge to 2 different very close reservoirs). At the bottom there is a line that is 'box = box_row.geometry'. If you have 2 bounding boxes you need choose one like this: box = box_row.geometry[0] or box = box_row.geometry[1] and make sure you get the right one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This reservoir will take every 2m instead of every 1m, because it is quite large.\n",
      "Number of depth intervals =  24\n",
      "Gauge ID =  sp-o10334\n",
      "Depth intervals =  24\n",
      "Polygon extent =  POLYGON ((145.8670096781719 -36.9333702602493, 146.2166602825768 -37.18621079469114, 146.0879946475787 -37.3641411931636, 145.7383440431738 -37.11130065872175, 145.8670096781719 -36.9333702602493))\n"
     ]
    }
   ],
   "source": [
    "csv = '90_Library/Eildon_198801_sp-o10334.csv'\n",
    "bad_rows = [0,1,3] #these are the bad rows you want to drop, like if the first depth slice image is bad, put 0. must be in square brackets\n",
    "\n",
    "#Get the ID of the gauge \n",
    "gauge_number_df = pd.read_csv(csv, nrows=1, escapechar='#')\n",
    "column2 = list(gauge_number_df)[1]\n",
    "gauge_number_df = gauge_number_df.rename(columns = {column2 : 'gauge_ID'})\n",
    "ID_str = gauge_number_df.at[0, 'gauge_ID']\n",
    "#ID_str = ID_str.astype(str)  #If this box returns an error it might be because the ID for that gauge is already a string. \n",
    "                             #If you get an error about strings you can hash this line out.\n",
    "                            #If you get a Key Error, unhash this line\n",
    "#Make depth duration curve\n",
    "gauge_data = pd.read_csv(csv,\n",
    "                error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                         parse_dates=['Timestamp'], \n",
    "                         index_col=('Timestamp'),\n",
    "                        date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0])) #Robbi wrote this line\n",
    "gauge_data = gauge_data.dropna()\n",
    "gauge_data = gauge_data.sort_values('Value')\n",
    "gauge_data['rownumber'] = np.arange(len(gauge_data))\n",
    "gauge_data['Exceedence'] = (1-(gauge_data.rownumber/len(gauge_data)))*100\n",
    "gauge_data = gauge_data.drop(columns=['Interpolation Type', 'Quality Code'])\n",
    "\n",
    "#Get the depth range and intervals\n",
    "depth_integers = gauge_data.astype(np.int64)\n",
    "max_depth = depth_integers.Value.max()\n",
    "min_depth = depth_integers.Value.min()\n",
    "integer_array = depth_integers.Value.unique()\n",
    "integer_list = integer_array.tolist()\n",
    "\n",
    "#Take every 2m of depth if the reservoir is large \n",
    "if len(integer_list) > 25:\n",
    "    integer_list2 = integer_list[::2]\n",
    "    print('This reservoir will take every 2m instead of every 1m, because it is quite large.')\n",
    "else:\n",
    "        integer_list2 = integer_list\n",
    "        print('This reservoir will take every 1m of depth')\n",
    "        \n",
    "print('Number of depth intervals = ', len(integer_list2)) \n",
    "\n",
    "#Get the bounding box for the satellite query\n",
    "bb_gdf = gpd.read_file('00_Lib_bound/00_Lib_bound.shp')\n",
    "bb_gdf = bb_gdf.set_index(['gauge_ID'])\n",
    "row_n = bb_gdf.index.get_loc(ID_str)\n",
    "box_row = bb_gdf.loc[ID_str]\n",
    "box = box_row.geometry#[0]  #If you are getting an error in the next box it might be because you are calling 2 bounding boxes\n",
    "                            #use [0] or [1] to just get one bounding box if you are getting an error about polygons\n",
    "\n",
    "print('Gauge ID = ', ID_str)\n",
    "print('Depth intervals = ', len(integer_list2))\n",
    "print('Polygon extent = ', box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load satellite data and make depth to surface area table\n",
    "If you have changed 'if len(integer_list) > 25:' to something other than 25, you need to change it in this box too.\n",
    "\n",
    "There is an option to plot images here, but I have them hashed out because I usually don't want to see the images again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = geometry.Geometry(geom=box, crs=bb_gdf.crs)\n",
    "query = {'time': ('01-01-1988', '23-02-2021')}\n",
    "query.update({'geopolygon': geom})\n",
    "\n",
    "dc = datacube.Datacube(app='dc-WOfS')\n",
    "\n",
    "wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, group_by='solar_day', fuse_func = wofs_fuser, **query)\n",
    "poly_mask = xr_rasterize(bb_gdf.geometry, wofs_albers)\n",
    "wofs_albers = wofs_albers.where(poly_mask, other=wofs_albers.water.nodata) #put other or all the data turns into 0\n",
    "\n",
    "gauge_data_xr = gauge_data.to_xarray() #convert gauge data to xarray\n",
    "merged_data = gauge_data_xr.interp(Timestamp=wofs_albers.time) #use xarrays .interp() function to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60332c578b04121903453a9a7a9dee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Depth</th>\n",
       "      <th>Surface Area</th>\n",
       "      <th>Number of images before masking</th>\n",
       "      <th>Number of images after masking</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243</td>\n",
       "      <td>18125</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245</td>\n",
       "      <td>25405000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>247</td>\n",
       "      <td>28440000</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249</td>\n",
       "      <td>26970625</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>251</td>\n",
       "      <td>35196250</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>253</td>\n",
       "      <td>39083750</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>255</td>\n",
       "      <td>43513125</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>257</td>\n",
       "      <td>46412500</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>259</td>\n",
       "      <td>50998750</td>\n",
       "      <td>62</td>\n",
       "      <td>34</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>261</td>\n",
       "      <td>54088125</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>263</td>\n",
       "      <td>58903125</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>265</td>\n",
       "      <td>62651250</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>267</td>\n",
       "      <td>66923125</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>269</td>\n",
       "      <td>71264375</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>271</td>\n",
       "      <td>75704375</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>273</td>\n",
       "      <td>80338125</td>\n",
       "      <td>34</td>\n",
       "      <td>19</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>275</td>\n",
       "      <td>85192500</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>277</td>\n",
       "      <td>91343125</td>\n",
       "      <td>49</td>\n",
       "      <td>25</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>279</td>\n",
       "      <td>95305625</td>\n",
       "      <td>59</td>\n",
       "      <td>27</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>281</td>\n",
       "      <td>100513750</td>\n",
       "      <td>54</td>\n",
       "      <td>27</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>283</td>\n",
       "      <td>106466250</td>\n",
       "      <td>64</td>\n",
       "      <td>44</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>285</td>\n",
       "      <td>112748125</td>\n",
       "      <td>34</td>\n",
       "      <td>22</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>287</td>\n",
       "      <td>117856250</td>\n",
       "      <td>53</td>\n",
       "      <td>33</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>289</td>\n",
       "      <td>92713750</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sp-o10334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Depth  Surface Area  Number of images before masking  \\\n",
       "0     243         18125                                3   \n",
       "1     245      25405000                                3   \n",
       "2     247      28440000                               11   \n",
       "3     249      26970625                                4   \n",
       "4     251      35196250                               18   \n",
       "5     253      39083750                               17   \n",
       "6     255      43513125                               21   \n",
       "7     257      46412500                               24   \n",
       "8     259      50998750                               62   \n",
       "9     261      54088125                               30   \n",
       "10    263      58903125                               28   \n",
       "11    265      62651250                               39   \n",
       "12    267      66923125                               46   \n",
       "13    269      71264375                               45   \n",
       "14    271      75704375                               30   \n",
       "15    273      80338125                               34   \n",
       "16    275      85192500                               45   \n",
       "17    277      91343125                               49   \n",
       "18    279      95305625                               59   \n",
       "19    281     100513750                               54   \n",
       "20    283     106466250                               64   \n",
       "21    285     112748125                               34   \n",
       "22    287     117856250                               53   \n",
       "23    289      92713750                                1   \n",
       "\n",
       "    Number of images after masking         ID  \n",
       "0                                2  sp-o10334  \n",
       "1                                3  sp-o10334  \n",
       "2                                7  sp-o10334  \n",
       "3                                1  sp-o10334  \n",
       "4                               12  sp-o10334  \n",
       "5                                6  sp-o10334  \n",
       "6                               11  sp-o10334  \n",
       "7                               14  sp-o10334  \n",
       "8                               34  sp-o10334  \n",
       "9                               16  sp-o10334  \n",
       "10                              16  sp-o10334  \n",
       "11                              27  sp-o10334  \n",
       "12                              24  sp-o10334  \n",
       "13                              27  sp-o10334  \n",
       "14                              17  sp-o10334  \n",
       "15                              19  sp-o10334  \n",
       "16                              24  sp-o10334  \n",
       "17                              25  sp-o10334  \n",
       "18                              27  sp-o10334  \n",
       "19                              27  sp-o10334  \n",
       "20                              44  sp-o10334  \n",
       "21                              22  sp-o10334  \n",
       "22                              33  sp-o10334  \n",
       "23                               1  sp-o10334  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surface_area_list = []\n",
    "number_of_images_list = []\n",
    "images_post_masking = []\n",
    "for i in tqdm(integer_list2, total=len(integer_list2)):\n",
    "    #If there are more than 25 meters in depth intervals, take every 2m depth\n",
    "    if len(integer_list) > 25:\n",
    "        specified_level = merged_data.where((merged_data.Value > i) & \n",
    "                                    (merged_data.Value < i+2), drop=True)\n",
    "    else:\n",
    "        specified_level = merged_data.where((merged_data.Value > i) & \n",
    "                                    (merged_data.Value < i+1), drop=True)\n",
    "    \n",
    "    date_list = specified_level.time.values\n",
    "    number_of_images = len(date_list)\n",
    "    number_of_images_list.append(number_of_images)\n",
    "    specified_passes = wofs_albers.sel(time=date_list).compute() #This .compute() Xarray function loads actual images\n",
    "    \n",
    "    #cloudmask (Claire Krause wrote this for me)\n",
    "    cc = masking.make_mask(specified_passes.water, cloud=True)\n",
    "    ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "    # Calculate the total number of pixels per timestep\n",
    "    npixels_per_slice = (specified_passes.water.shape[1] * \n",
    "                         specified_passes.water.shape[2])\n",
    "    cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "    clear_specified_passes = specified_passes.water.isel(\n",
    "        time=cloud_pixels_fraction < 0.2)\n",
    "    images_post_masking.append(len(clear_specified_passes.time))\n",
    "    wet = masking.make_mask(clear_specified_passes, wet=True).sum(dim='time')\n",
    "    dry = masking.make_mask(clear_specified_passes, dry=True).sum(dim='time')\n",
    "    clear = wet + dry\n",
    "    frequency = wet / clear\n",
    "    frequency = frequency.fillna(0)  \n",
    "    \n",
    "    #Get area from the satellite data\n",
    "    #get the frequency array\n",
    "    frequency_array = frequency.values\n",
    "    #Turn any pixel in the frequency array with a value greater than 0.2 into a pixel of value 1\n",
    "    #if the pixel value is 0.2 or lower it gets value 0\n",
    "    is_water = np.where((frequency_array > 0.2),1,0)\n",
    "    #give the 'frequency' xarray back its new values of zero and one\n",
    "    frequency.values = is_water\n",
    "    #sum up the pixels\n",
    "    number_water_pixels = frequency.sum(dim=['x', 'y'])\n",
    "    #get the number\n",
    "    number_water_pixels = number_water_pixels.values.tolist()\n",
    "    #multiply by pixel size to get area in m2\n",
    "    area_m2 = number_water_pixels*(25*25)\n",
    "    \n",
    "    surface_area_list.append(area_m2)\n",
    "    \n",
    "        #Plotting the image (keep these hashed out if you don't want to see images)\n",
    "    #fig = plt.figure()\n",
    "    #frequency.plot(figsize = (7,5))\n",
    "    #name = ID_str, i\n",
    "    #images = plt.savefig('images/'+str(name)+'.png')\n",
    "    #plt.close(fig)\n",
    "\n",
    "    \n",
    "              \n",
    "#create dataframe of depth to surface area \n",
    "depth_to_area_df = DataFrame(integer_list2, columns=['Depth'])\n",
    "depth_to_area_df['Surface Area'] = surface_area_list\n",
    "depth_to_area_df['Number of images before masking'] = number_of_images_list\n",
    "depth_to_area_df['Number of images after masking'] = images_post_masking\n",
    "depth_to_area_df['ID'] = ID_str\n",
    "depth_to_area_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add highest surface area from polygon\n",
    "The polygons from DEA's Hydropoly dataset can be used to calculate the maximum possible surface area. The area from 'ID_and_area' csv file was calculated from the polygon shapefiles and is in m2. The maximum depth in Australian Height Datum (m above sea level) has to also be obtaned. The problem is that I haven't got the max height of all the reservoirs, I don't know of a dataset that has that information. This method will be more correct than extrapolating the heighest depths. \n",
    "\n",
    "Note: I've noticed that the polygon areas do not always match the surface area given by ANCOLD. I trust the polygon calculated areas more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_df = pd.read_csv('ID_and_area.csv', index_col = 'gauge_ID')\n",
    "area_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_number_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate\n",
    "This is the part where images showing lower surface area as depth increases get thrown out and instead the code will give them a value by interpolation. This is also where the bad_rows get taken into account. If you hashed out the bad_rows line in the first box, you will have to hash out the line here that uses that variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_list = []\n",
    "for i, obs in depth_to_area_df.iterrows():\n",
    "    if i == 0: #We always put the first point in, even if it's bad (will fix later)\n",
    "        obs_list.append(obs)\n",
    "        continue\n",
    "    if obs[\"Surface Area\"] > obs_list[-1][\"Surface Area\"]: # if the number is bigger than the number before it, keep.\n",
    "        obs_list.append(obs)\n",
    "\n",
    "obs_df = pd.DataFrame(obs_list)\n",
    "obs_df2 = obs_df\n",
    "\n",
    "#drop the first real images that are bad\n",
    "obs_df2 = obs_df.drop(index=bad_rows) #Delete rows made from bad images\n",
    "\n",
    "obs_df3 = obs_df2.set_index(\"Depth\")\n",
    "min_ = obs_df3.index.min()\n",
    "max_ = obs_df3.index.max()\n",
    "obs_df4 = obs_df3.reindex(range(min_, max_+1))\n",
    "obs_df4['Surface Area'] = obs_df4['Surface Area'].interpolate()\n",
    "obs_df4['Depth'] = obs_df4.index\n",
    "obs_df4['ID'] = obs_df3.at[min_, 'ID']\n",
    "obs_df4 = obs_df4[['ID', 'Depth', 'Surface Area', 'Number of images after masking']]\n",
    "obs_df4 = obs_df4.set_index(\"ID\")\n",
    "obs_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate higher depths\n",
    "The Interpolate function won't get the highest or lowest depths, as it can only get the inbetween depths. To get the lowers and higher depths I have used a simple polynomial (y = mx + b) to calculate what the curve might look like at the extremities of the depths. This might not be super super accurate, but it's probably close enough, and there won't be many days when the gauge reads these levels anyway, so error will be minimised for our purposes. The code block after this one will show you the curve with the extrapolated values. You can alter this code if you think the curve doesn't look quite right (eg if there is negative surface areas in the lower extrapolations). An image of this graph will be saved in the images folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_orig = list(obs_df4['Surface Area'])\n",
    "x_orig = list(obs_df4['Depth'])\n",
    "ID = list(obs_df2.index)\n",
    "ID = str(ID[0])\n",
    "\n",
    "#Top point\n",
    "x2 = x_orig[-2]\n",
    "y2 = y_orig[-2]\n",
    "\n",
    "#Second top point\n",
    "x1 = x_orig[-1]\n",
    "y1 = y_orig[-1]\n",
    "m = (y2 - y1)/(x2 - x1)\n",
    "b = y1-(m*x1)\n",
    "\n",
    "#Get an extra 5m at the top\n",
    "x_list = [x1, x1+1, x1+2, x1+3, x1+4, x1+5]\n",
    "\n",
    "\n",
    "#Use the first degree polynomial to find y (surface area)\n",
    "y_list = []    \n",
    "for i in x_list:\n",
    "    y = (m*i)+b # y = mx + b\n",
    "    y_list.append(y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate lower depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowest point\n",
    "x1 = x_orig[0]\n",
    "y1 = y_orig[0]\n",
    "\n",
    "#Second lowest point\n",
    "x2 = x_orig[1]\n",
    "y2 = y_orig[1]\n",
    "\n",
    "m = (y2 - y1)/(x2 - x1)\n",
    "b = y1-(m*x1)\n",
    "\n",
    "#Get an extra 5m at the bottom\n",
    "x_list2 = [x1, x1-1, x1-2]#, x1-3, x1-4, x1-5]\n",
    "\n",
    "\n",
    "#Use the first degree polynomial to find y (surface area)\n",
    "y_list2 = []    \n",
    "for i in x_list2:\n",
    "    y = (m*i)+b # y = mx + b\n",
    "    y_list2.append(y)\n",
    "    \n",
    "plt.title(ID_str)\n",
    "plt.plot(x_orig, y_orig)\n",
    "plt.plot(x_list, y_list)\n",
    "plt.plot(x_list2, y_list2, color='red')\n",
    "plt.savefig('images/'+str(ID_str)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add higher and lower depths onto the original dataframe\n",
    "This code block makes your final depth to surface area table, which you can save out as a csv file. The 'Number of images after masking' column tells you how many images were stacked to calculate the final surface area. Anything with 10 or more images, I would consider quite accurate. If there's only 1 or 2 images, it could still be accurate depending on the pass, but you should check the image and make a decision. NaN values mean there were no images used in the calculation of that surface area, and the value was derived from either interpolation or extrapolation with the polynomial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Higher depths\n",
    "high_data = x_list, y_list\n",
    "df_high = pd.DataFrame(data=high_data)\n",
    "df_high = df_high.transpose()\n",
    "df_high2 = df_high.rename(columns={0:'Depth', 1:'Surface_Area'})\n",
    "depth_exp_high = df_high2.Depth.explode()\n",
    "surface_exp_high = df_high2.Surface_Area.explode()\n",
    "surface_exp_high = surface_exp_high.to_frame()\n",
    "depth_exp_high = depth_exp_high.to_frame()\n",
    "depth_exp_high['Surface Area'] = surface_exp_high\n",
    "depth_exp_high['ID'] = ID_str\n",
    "depth_exp_high = depth_exp_high.set_index(\"ID\")\n",
    "\n",
    "#Lower depths\n",
    "low_data = x_list2, y_list2\n",
    "df_low = pd.DataFrame(data=low_data)\n",
    "df_low = df_low.transpose()\n",
    "df_low2 = df_low.rename(columns={0:'Depth', 1:'Surface_Area'})\n",
    "depth_exp_low = df_low2.Depth.explode()\n",
    "surface_exp_low = df_low2.Surface_Area.explode()\n",
    "surface_exp_low = surface_exp_low.to_frame()\n",
    "depth_exp_low = depth_exp_low.to_frame()\n",
    "depth_exp_low['Surface Area'] = surface_exp_low\n",
    "depth_exp_low['ID'] = ID_str\n",
    "depth_exp_low = depth_exp_low.set_index(\"ID\")\n",
    "\n",
    "#Add to Interpolated df\n",
    "full_df = obs_df4.append(depth_exp_low)\n",
    "full_df2 = full_df.append(depth_exp_high)\n",
    "full_df2 = full_df2.sort_values(by=['Depth'])\n",
    "full_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the final table as a csv\n",
    "The file will save onto the Sandbox and then you can download it onto your local machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df2.to_csv('corrected_dataframes/'+ID_str +'-corrected'+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
