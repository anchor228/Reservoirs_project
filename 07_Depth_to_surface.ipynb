{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a depth to surface area relationship for a reservoir\n",
    "This notebook will create a table of the depth to surface area relationship for a reservoir. You can then know what the surface area is based on the depth gauge reading. Read the instructions carefully and you should be able to create your own table for any of the reservoirs in the 00_Library or 90_Library folders.\n",
    "\n",
    "Inputs: \n",
    "* csv file of storage level data downloaded from http://www.bom.gov.au/waterdata/\n",
    "* A shapefile of all the reservoirs with their gauge IDs attached (I made one in ArcGIS from the National Hydropolys dataset)\n",
    "\n",
    "Outputs:\n",
    "* Images of the reservoir at each 1m depth slice\n",
    "* A csv file of the depth the surface area table\n",
    "\n",
    "Select a code block and press 'Shift' + 'Enter' to run. They have to be run in order. Let's start with the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules we will need. This includes modules by DEA to help manage the OpenDataCube.\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm.auto import tqdm #this one is a loading bar, it's cool to add loading bars to loops\n",
    "import rasterio.crs\n",
    "from pandas import DataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import datacube\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../Scripts')\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from dea_datahandling import wofs_fuser #this joins wofs data across tiles correctly\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.utils import masking\n",
    "from datacube.helpers import ga_pq_fuser, write_geotiff\n",
    "#from digitalearthau.utils import wofs_fuser\n",
    "#import DEAPlotting, DEADataHandling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs\n",
    "Put the location of your csv file and the reservoirs shapefile in here. All of the file names are in caps and you can press tab to fill the rest of the file name. Check out the Library folder to see the gauge data I have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User inputs\n",
    "csv = 'Library/MANGROVE_CREEK_DAM_MANGRV.1.csv'\n",
    "reservoirs_shape_file = 'Named_Reservoirs/Named_Reservoirs8.shp' #This shapefile has the gauge ID attached to the polygons\n",
    "todays_date = '11-05-2021' #for most up to date WOFS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find corresponding reservoir polygon\n",
    "If you get an error about 'str' object has no attribute 'astype', you need to hash out the line indicated, or if it's hashed out and you get an error, unhash it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the name and ID of the gauge \n",
    "gauge_number_df = pd.read_csv(csv, nrows=1, escapechar='#')\n",
    "column2 = list(gauge_number_df)[1]\n",
    "gauge_number_df = gauge_number_df.rename(columns = {column2 : 'gauge_ID'})\n",
    "ID_str = gauge_number_df.at[0, 'gauge_ID']\n",
    "#ID_str = ID_str.astype(str) #hash/unhash this line out if you get an error\n",
    "# Read the reservoirs shapefile\n",
    "gdf = gpd.read_file(reservoirs_shape_file)\n",
    "gdf = gdf.set_index(['gauge_ID'])\n",
    "\n",
    "row = gdf.loc[ID_str]\n",
    "polygon = row.geometry\n",
    "print('The gauge ID is ', ID_str)\n",
    "print('Here is a picture of the reservoir polygon:')\n",
    "polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query satellite data\n",
    "Now we'll query all of the WOFS data ever for inside that polygon. We'll mask out anything outside the polygon because we don't want other waterbodies getting counted in the image.\n",
    "\n",
    "This block has the dc.load() function in it (dc stands for datacube). Usually this loads the satellite images from the Open DataCube, but I put a dask_chunks argument in there so it won't load the images yet, it will just load the parameters, because that's way faster and that's enough to link the data. This cell can take a minute to run, longer for large dams. You'll know when it's finished when the square brackets in the top left stop being a star and become a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {'time': ('01-01-1988', todays_date)}\n",
    "\n",
    "dc = datacube.Datacube(app='dc-WOfS') #this is how you access the open data cube where the satellite data is\n",
    "\n",
    "geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "query.update({'geopolygon': geom})\n",
    "\n",
    "#Load parameters of satellite data in preparation to merge it with gauge data\n",
    "wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, \n",
    "                     group_by='solar_day', fuse_func = wofs_fuser, **query) #wofs_fuser is important, it fixes thing on the edge of tiles\n",
    "\n",
    "# Mask out anything outside the reservoir extent\n",
    "poly_mask = xr_rasterize(gdf.geometry, wofs_albers)\n",
    "wofs_albers = wofs_albers.where(poly_mask, other=wofs_albers.water.nodata) #put other argument or all the data turns into 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge depth gauge data with satellite data\n",
    "We can link the gauge data and satellite data together on the time axis. Then we can call the satellite passes by depth, instead of just by date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the gauge data\n",
    "data = pd.read_csv(csv, error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                     parse_dates=['Timestamp'], \n",
    "                     index_col=('Timestamp'),\n",
    "                    date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0]))\n",
    "gauge_data = data.drop(columns=['Quality Code', 'Interpolation Type'])\n",
    "\n",
    "#Merge the gauge data with the satellite data\n",
    "gauge_data_xr = gauge_data.to_xarray() #convert gauge data to xarray\n",
    "merged_data = gauge_data_xr.interp(Timestamp=wofs_albers.time) #use xarrays .interp() function to merge\n",
    "gauge_data = gauge_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise the depth gauge data into slices of depth\n",
    "This cell also has a part in it where it takes every 2m instead of every 1m if the reservoir is really deep (goes up and down more than 25m). This makes it more accurate in my opinion because you get more passes per depth interval, so the images that get generated are more accurate. You can just interpolate every other meter later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the depth range and intervals\n",
    "depth_integers = gauge_data.astype(np.int64)\n",
    "max_depth = depth_integers.Value.max()\n",
    "min_depth = depth_integers.Value.min()\n",
    "integer_array = depth_integers.Value.unique()\n",
    "integer_list = integer_array.tolist()\n",
    "\n",
    "#Take every 2m of depth if the reservoir is large \n",
    "if len(integer_list) > 25:\n",
    "    integer_list2 = integer_list[::2]\n",
    "    print('This reservoir will take every 2m instead of every 1m, because it is quite large.')\n",
    "else:\n",
    "        integer_list2 = integer_list\n",
    "        print('This reservoir will take every 1m of depth')\n",
    "        \n",
    "print('Number of depth intervals = ', len(integer_list2))\n",
    "integer_list.sort()\n",
    "integer_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an image of the reservoir at each meter of depth\n",
    "Here's the part where the satellite data gets called out of the datacube and organised into depth slices. In you files on the left hand side there you should see a folder called images. That's where all the pictures will get saved as png files. Download those images to your local machine and then I suggest delete them off the sandbox because they just take up space and you don't need them for anything else. This cell will take a bit longer to run because it has to load a lot of satellite passes and cload mask them all.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_area_list = []\n",
    "number_of_images_list = []\n",
    "images_post_masking = []\n",
    "for i in tqdm(integer_list2, total=len(integer_list2)):\n",
    "    #If there are more than 25 meters in depth intervals, take every 2m depth\n",
    "    if len(integer_list) > 25:\n",
    "        specified_level = merged_data.where((merged_data.Value > i) & \n",
    "                                    (merged_data.Value < i+2), drop=True)\n",
    "    else:\n",
    "        specified_level = merged_data.where((merged_data.Value > i) & \n",
    "                                    (merged_data.Value < i+1), drop=True)\n",
    "    \n",
    "    date_list = specified_level.time.values[:250]\n",
    "    number_of_images = len(date_list)\n",
    "    number_of_images_list.append(number_of_images)\n",
    "    specified_passes = wofs_albers.sel(time=date_list).compute() #This .compute() Xarray function loads actual images\n",
    "    \n",
    "    #cloudmask (Claire Krause wrote this for me)\n",
    "    cc = masking.make_mask(specified_passes.water, cloud=True)\n",
    "    ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "    # Calculate the total number of pixels per timestep\n",
    "    npixels_per_slice = (specified_passes.water.shape[1] * \n",
    "                         specified_passes.water.shape[2])\n",
    "    cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "    clear_specified_passes = specified_passes.water.isel(\n",
    "        time=cloud_pixels_fraction < 0.2)\n",
    "    images_post_masking.append(len(clear_specified_passes.time))\n",
    "    wet = masking.make_mask(clear_specified_passes, wet=True).sum(dim='time')\n",
    "    dry = masking.make_mask(clear_specified_passes, dry=True).sum(dim='time')\n",
    "    clear = wet + dry\n",
    "    frequency = wet / clear\n",
    "    frequency = frequency.fillna(0)  \n",
    "    \n",
    "    #Get area from the satellite data\n",
    "    #get the frequency array\n",
    "    frequency_array = frequency.values\n",
    "    #Turn any pixel in the frequency array with a value greater than 0.2 into a pixel of value 1\n",
    "    #if the pixel value is 0.2 or lower it gets value 0\n",
    "    is_water = np.where((frequency_array > 0.2),1,0)\n",
    "    #give the 'frequency' xarray back its new values of zero and one\n",
    "    frequency.values = is_water\n",
    "    #sum up the pixels\n",
    "    number_water_pixels = frequency.sum(dim=['x', 'y'])\n",
    "    #get the number\n",
    "    number_water_pixels = number_water_pixels.values.tolist()\n",
    "    #multiply by pixel size to get area in m2\n",
    "    area_m2 = number_water_pixels*(25*25)\n",
    "    \n",
    "    surface_area_list.append(area_m2)\n",
    "    \n",
    "        #Plotting the image (keep these hashed out if you don't want to see images)\n",
    "    fig = plt.figure()\n",
    "    frequency.plot(figsize = (7,5))\n",
    "    name = ID_str, i\n",
    "    images = plt.savefig('images/'+str(name)+'.png')\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a table of the depth to surface area relationship\n",
    "Nice, you made image slices of the reservoir! Now let's make the table. You might notice that some of the images are blank or bad quality and therefore produce an inaccurate calculation of the surface area. The reason this happens is because there weren't many cloud free passes for that depth over the last 30 years. It's OK, we can fix it later. If there are outliers in the gauge data csv file, you will get a lot of zero values on the lowest or highest depths because they aren't real. These get deleted later. Anyway, let's see what the initial depth to surface area table looks like and then next we'll fix it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of depth to surface area \n",
    "depth_to_area_df = DataFrame(integer_list2, columns=['Depth'])\n",
    "depth_to_area_df['Surface Area'] = surface_area_list\n",
    "depth_to_area_df['pre masking'] = number_of_images_list\n",
    "depth_to_area_df['post masking'] = images_post_masking\n",
    "depth_to_area_df['ID'] = ID_str\n",
    "depth_to_area_df = depth_to_area_df.sort_values(by=['Depth'])\n",
    "depth_to_area_df = depth_to_area_df.reset_index()\n",
    "depth_to_area_df = depth_to_area_df[['ID', 'Depth','Surface Area', 'pre masking', 'post masking']]\n",
    "depth_to_area_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix with interpolation\n",
    "If you got some outliers in the depth gauge data or any dodgy images/surface area calculations, we can fix them here. This block basically says 'if the surface area calculation is zero, or smaller than the one before it, delete' because it doesn't make sense for surface area to go down if depth goes up. It will then interpolate the surface area values instead. If you did a large reservoir like Eucumbene or something, this is where every other meter gets its surface area calculated. The only thing this code won't fix is if your first non-zero value (ie lowest depth you have an image for) is a bad quality image (for example the surface area might be calculated as 20'000m2 from an image with scan lines through it, when it should have been 150'000m2) but if your first non-zero image looks ok to you then you're good. This code also won't interpolate the very highest depth if it's a bad image, it will just delete it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_list = []\n",
    "for i, obs in depth_to_area_df.iterrows():\n",
    "    if i == 0: #We always put the first point in, even if it's bad (will fix later)\n",
    "        obs_list.append(obs)\n",
    "        continue\n",
    "    if obs[\"Surface Area\"] > obs_list[-1][\"Surface Area\"]: # if the number is bigger than the number before it, keep.\n",
    "        obs_list.append(obs)\n",
    "\n",
    "obs_df = pd.DataFrame(obs_list)\n",
    "obs_df2 = obs_df\n",
    "obs_df2.drop(obs_df2[obs_df2['Surface Area'] == 0].index, inplace=True) #drop zero values because they stuff up the interpolation\n",
    "obs_df3 = obs_df2.set_index(\"Depth\")\n",
    "min_ = obs_df3.index.min()\n",
    "max_ = obs_df3.index.max()\n",
    "obs_df4 = obs_df3.reindex(range(min_, max_+1))\n",
    "obs_df4['Surface Area'] = obs_df4['Surface Area'].interpolate()\n",
    "obs_df4['Depth'] = obs_df4.index\n",
    "obs_df4['ID'] = obs_df3.at[min_, 'ID']\n",
    "obs_df4 = obs_df4[['ID', 'Depth', 'Surface Area']]\n",
    "obs_df4 = obs_df4.set_index(\"ID\")\n",
    "obs_df4 = obs_df4.sort_values(by='Depth')\n",
    "obs_df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the table as a csv file\n",
    "OK that should look better now. You can save this table as a csv and download it to your local machine. Don't forget you can save the images too, they are in the images folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df4.to_csv('images/'+ID_str +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
