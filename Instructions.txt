05/03/2021
The aim of this project is to create a lookup table of depth to surface area for all the reservoirs in Australia (or as many as possible). Here is the method I have used. 

1. Make your reservoirs shapefile: ArcMap, 01_Create_shapefile_of_Reservoirs.ipynb
Get the national hydropolys dataset from DEA. This is a shapefile that's publically available, so go look for it online and download it. in ArcMAP, select the polygon layer that has been labelled as 'Reservoirs' and save that out as it's own shapefile. Then you want to only get the reservoirs that have names attached to them (run the shapefile through 01_Create_shapefile_of_Reservoirs.ipynb to do this bit for you). 

2. Make your gauges shapefile: 02_Match_gauges_to_Reservoirs.ipynb
You gotta get all of the Storage Level gauges from http://www.bom.gov.au/waterdata/. I used a webscraping script called dea-bom_storage.py, which is in the Scripts folder. This script was recycled from a project I did when I was working for DEA back in 2019. Kirill wrote it for me (he's a developer at DEA). I modified it a bit for this project, but basically it goes into the water data online website and returns the location of the 'Storage Level' gauges from all over Australia. To run this script and generate the shapefile, run '02_Match_gauges_to_Reservoirs.ipynb', it will do everything for you hopefully. It makes you a shapefile of points representing the gauge locations.

3. Spatial Join the gauges shapefile with the reservoirs shapefile: ArcMAP
Now you have a shapefile of gauges and you have a shapefile of reservoirs. Save them out and open them in ArcMAP and do a spatial join. The idea is to make a shapefile of reservoir polygons that have the gauge ID attached to each one. You won't get every single reservoir matched to every single gauge but you should get a couple hundred matched up, which is pretty good. I've done this, the shapefile is in Spatial_Join_gauges_to_polys.

4. Make a library of good quality gauge data: www.bom.gov.au/waterdata/ 
This part is arduous (lucky for you I've already done it, and I've put all the good gauge data in 00_Library). Not every single reservoir in Australia is going to have good gauge data, so you have to manually go and check every single reservoir on the waterdata online website your self and make a decision on the quality of the gauge data and decide if you're going to include it or not. While doing this, you want to have a list of reservoirs printed out infront of you that you want data for. We used the ANCOLD list of reservoirs, for which there are about 500. Go to http://www.bom.gov.au/waterdata/ and manually search for 'Storage Level' gauges for each reservoir on the list and check if the hydrograph data is sufficient ('sufficient' for me meant data that went back to at least the year 2000 to get enough spread for the depth to surface area relationship). Download the good gauges as csv files and organise them neatly in a folder, which will be your library (see 00_Library). The csv file names should contain the name of the reservoir and the gauge ID, for example 'Wivenhoe_143036A.csv'. Each time you get a good csv file of gauge data, cross it off the list. When you're done making the reservoirs shapefile with the gauge IDs attached, upload it back into the Sandbox.

5. Make a shapefile of reservoirs representing only those which you have the good data for: 03_00_Library_reservoirs.ipynb
So by now you should have a few hundred reservoirs that have been matched up to a few hundred gauges, and you have a nice library of good quality data. Good job. This step just trims your shapefile down to only the reservoirs that you have good gauge data for. Just run your spatial join product through this notebook.

6. Make a shapefile of bounding boxes for the reservoirs: ArcMAP
Soon, you will want to get the satellite data for each reservoir so you can calculate surface area for each depth. When you have hundreds of reservoirs, you don't want to have to manually put in the extents of each reservoir so we'll make a shapefile of bounding boxes for each reservoir that can be used to call the satellite data for each reservoir. Make the bounding box shapefile in ArcMAP from the reservoirs shapefile you just made, it's pretty easy. The bounding boxes obviously have to have the gauge IDs attached to them. When you are done making a bounding box shapefile in ArcMAP that has the gauge IDs, upload it back into the sandbox.

7. OPTIONAL Generate the first iteration of the depth-to-surface area lookup tables: 04_query_with_bounding_boxes.ipynb
Now you have all the shapefiles and csv files you need to make the depth to surface area relationship for a couple hundred gauges. Run '04_query_with_bounding_boxes.ipynb' to load the satellite data, get the surface areas for each reservoir at each m of depth and generate the depth-to-surface area lookup table. The table it makes is a csv file so you can download it onto your computer and open it in excel. 

8. Generate images for every depth interval for each reservoir: 05_Generate_image_profiles.ipynb
I have run this and I do have the images on my local drive but I've deleted them off my Sandbox account. I can zip file them to you if you want, or you can just run this notebook and regenerate them. You'll see with the first iteration that most of the reservoirs have a pretty good relationship, like the satellite images worked and you could see an image of the reservoir at each depth. However there are some images that are bad quality or missing, meaning the calculated surface area won't be right.  It's important that a human looks at every single one of the images for every single reservoir, for quality control. So in this step, we generate those images and save them as png files. I suggest downloading all of these images and saving them in ordered folders on your local computer so you can look at every single one in a systematic way. Delete the images off the sandbox once you download them because they are just gonna take up space and you don't need them anymore. If you want to generate the images for just one reservoir at a time instead of all of them, use 06_Fix_one_at_a_time.ipynb. 

9. Generate the second iteration of the depth-to-surface area lookup tables: 06_Fix_one_at_a_time.ipynb
Now that you can visually assess each image, you can make the final, quality controlled depth to surface area table. Each reservoir has to be done individually because you have to look at each one. Follow the markdowns in 06_Fix_one_at_a_time.ipynb. Basically, this notebook will delete any values that don't make sense and then interpolate them from the good values. It will also extrapolate the depth to surface curve to estimate higher and lower depths that there are no clear satellite images for. It is meant to be a very flexable notebook, so if the extrpolations or anything doesn't seem right, you can edit the code until you're happy with the output. You might want to use data that goes back further than the year 2000, for example Eucumbene was fuller in the 90s than it ever has been, so you need to go back to the 90s to get these passes that occured at higher depths (see 90_Library). You might want to use every second meter, or every 3m instead of 1m. You might even decide it's better to just use a static polygon to calculate one surface area (for example lake Tuggeranong and Lake Ginenderra only go up and down by 2m, so may as well use a polygon). I have gone through almost 100 reservoirs in this way and the final tables are in corrected_dataframes. These tables need to be validated against other data. There are some issues that I am aware of: 1.bridges obscuring water. 2.In Tasmania in the great lakes region, the spatial joins didn't really work because the reservoirs are so close to each other. 3.Some images need to be cropped because they contain parts of other reservoirs or dams. Overall though I'm happy with the results and most reservoirs look they have a good depth to surface area relationship from this project. 

10. Concatenate all these tables into one dataset
I haven't done this yet but I'm sure you can figure it out if you know how to code with Pandas or something. All the files are stored in corrected_dataframes. I've also made a csv file that has the names of the reservoirs (as they were in the hydrolpolys dataset) attached tho their gauge IDs called names_and_IDs.csv, incase you want to include the names of the reservoirs. 