{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call WOfS using bounding boxes\n",
    "This notebook merges gauge data with satellite data for multiple reservoirs at a time (in the 00_Library diresctory). It also makes the depth to surface area relatiosnhips for the reservoirs. It's my main notebook for this project and closest to what I have as the final product. Bounding boxes are used to get the exact extents of reservoirs for the depth-to-surface area relationship. It's an automated way to call the right satellite data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/cligj/__init__.py:17: FutureWarning: cligj 1.0.0 will require Python >= 3.7\n",
      "  warn(\"cligj 1.0.0 will require Python >= 3.7\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import rasterio.crs\n",
    "from tqdm.auto import tqdm #this one is a loading bar, it's cool to add loading bars to loops\n",
    "from pandas import DataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import datacube\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../Scripts')\n",
    "from dea_spatialtools import xr_rasterize\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.utils import masking\n",
    "from datacube.helpers import ga_pq_fuser, write_geotiff\n",
    "#from digitalearthau.utils import wofs_fuser\n",
    "#import DEAPlotting, DEADataHandling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask load the satellite data for all the reservoirs\n",
    "I made a shapefile in ArcMAP that has bounding boxes of the reservoirs identified in 00_Library_reservoirs. This is what we will use for the query, so wofs knows where to make the picture for each reservoir. The following code blocks were copied from a DEA notebook called 'Open and run analysis on multiple polygons'. In this case the multiple polygons are my bounding boxes from the geodataframe above. First you make a query with no x, y points and no CRS. Just the time. Then you loop the location for the query using a datacube package called geomoetry. Put the dc.load() line in the loop (I'm going to dask load, not load actual images, I'll do that later after I've merged with the gauges). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1aed74492c94aa5a2ba03bb34dfd5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=153.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('00_Lib_bound/00_Lib_bound.shp')\n",
    "\n",
    "query = {'time': ('01-01-1988', '09-12-2020')} \n",
    "         #'crs': 'EPSG:3577'}\n",
    "dc = datacube.Datacube(app='dc-WOfS')\n",
    "\n",
    "results = {} \n",
    "\n",
    "#tqdm is gonna make the bar. tqdm is Arabic abbreviation for 'progress'\n",
    "for index, row in tqdm(gdf.iterrows(), total=len(gdf)):\n",
    "    geom = geometry.Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "    query.update({'geopolygon': geom})\n",
    "    \n",
    "    wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, group_by='solar_day', **query)\n",
    "    \n",
    "    poly_mask = xr_rasterize(gdf.iloc[[index]], wofs_albers)\n",
    "    wofs_albers = wofs_albers.where(poly_mask, other=wofs_albers.water.nodata) #put other or all the data turns into 0\n",
    "    \n",
    "    results.update({str(row['gauge_ID']): wofs_albers}) #The handle for dictionary objects is the gauge ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop read all the csv files in 00_Library\n",
    "Now we have a library of the wofs data with the gauge ID as the key. We now need a library of the depth data with, again, the gauge ID as the key. Then we can match them up later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of the file names so we can call them with pandas\n",
    "file_list = []\n",
    "\n",
    "directory = '00_Library'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_list.append(os.path.join(directory, filename))\n",
    "\n",
    "#Read the gauge files twice, once to get ID and second to get the data. Append them together in a dictionary\n",
    "#May as well make a list of IDs here because we will probably use it later\n",
    "data_dict = {}        \n",
    "ID_list = []\n",
    "#let's use tqdm again to make a progress bar. The bar is so cool I love this module\n",
    "#I'm gonna use tqdm on literally all of my loops now\n",
    "for i in tqdm(file_list, total=len(file_list)):\n",
    "    df = pd.read_csv(i, nrows=1, escapechar='#')\n",
    "    column = df.iloc[:,[1]] #This is the column with the ID in it\n",
    "    ID = list(column)\n",
    "    ID = ID[0]\n",
    "    ID = df.at[0, ID]\n",
    "    ID_list.append(str(ID))\n",
    "    \n",
    "    data = pd.read_csv(i, error_bad_lines = False, skiprows=9, escapechar='#',\n",
    "                         parse_dates=['Timestamp'], \n",
    "                         index_col=('Timestamp'),\n",
    "                        date_parser=lambda x: pd.to_datetime(x.rsplit('+', 1)[0]))\n",
    "    data = data.drop(columns=['Quality Code', 'Interpolation Type'])\n",
    "    data_dict.update({str(ID): data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a function that generates the depth slices of a reservoir and gets the depth to surface area relationship\n",
    "This is my first time writing a function, Matthew and Bex from DEA helped me write it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prod(ID_caller, gauge_data, wofs_albers, make_plots = False) -> 'depth slices': \n",
    "    \"\"\"\n",
    "    This function takes the gauge data and the wofs data,\n",
    "    cloud masks the images and counts the pixels in each depth slice.\n",
    "    It returns a list of all the surface areas per depth.\n",
    "    \n",
    "    \"\"\"\n",
    "    #Get the depth range and intervals\n",
    "    gauge_data = gauge_data.dropna()\n",
    "    depth_integers = gauge_data.astype(np.int64)\n",
    "    max_depth = depth_integers.Value.max()\n",
    "    min_depth = depth_integers.Value.min()\n",
    "    integer_array = depth_integers.Value.unique()\n",
    "    integer_list = integer_array.tolist()\n",
    "    \n",
    "    gauge_data_xr = gauge_data.to_xarray() #convert gauge data to xarray\n",
    "    merged_data = gauge_data_xr.interp(Timestamp=wofs_albers.time) #use xarrays .interp() function to merge\n",
    "\n",
    "    surface_area_list = []\n",
    "\n",
    "    for i in tqdm(integer_list, leave = False):\n",
    "        specified_level = merged_data.where((merged_data.Value > i) & \n",
    "                                        (merged_data.Value < i+1), drop=True)\n",
    "        date_list = specified_level.time.values[:150] #caps images at x per slice (way faster)\n",
    "        n_images_used = int(len(date_list))\n",
    "        specified_passes = wofs_albers.sel(time=date_list).compute() #This .compute() Xarray function loads actual images\n",
    "        #cloudmask (Claire Krause wrote this for me)\n",
    "        #print(specified_passes.water)\n",
    "        cc = masking.make_mask(specified_passes.water, cloud=True)\n",
    "        ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "        # Calculate the total number of pixels per timestep\n",
    "        npixels_per_slice = (specified_passes.water.shape[1] * \n",
    "                             specified_passes.water.shape[2])\n",
    "        cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "        clear_specified_passes = specified_passes.water.isel(\n",
    "            time=cloud_pixels_fraction < 0.2)\n",
    "        wet = masking.make_mask(clear_specified_passes, wet=True).sum(dim='time')\n",
    "        dry = masking.make_mask(clear_specified_passes, dry=True).sum(dim='time')\n",
    "        clear = wet + dry\n",
    "        frequency = wet / clear\n",
    "        frequency = frequency.fillna(0)  \n",
    "\n",
    "        #Get area from the satellite data\n",
    "        #get the frequency array\n",
    "        frequency_array = frequency.values\n",
    "        #Turn any pixel in the frequency array with a value greater than 0.2 into a pixel of value 1\n",
    "        #if the pixel value is 0.2 or lower it gets value 0\n",
    "        is_water = np.where((frequency_array > 0.2),1,0)\n",
    "        #give the 'frequency' xarray back its new values of zero and one\n",
    "        frequency.values = is_water\n",
    "        #sum up the pixels\n",
    "        number_water_pixels = frequency.sum(dim=['x', 'y'])\n",
    "        #get the number\n",
    "        number_water_pixels = number_water_pixels.values.tolist()\n",
    "        #multiply by pixel size to get area in m2\n",
    "        area_m2 = number_water_pixels*(25*25)\n",
    "\n",
    "        surface_area_list.append([ID_caller, i, area_m2, n_images_used])\n",
    "        #print('This is the area as calculated from wet pixels at', i, 'meters', area_m2)\n",
    "\n",
    "        #Plotting the image\n",
    "        if make_plots:\n",
    "            frequency.plot(figsize = (7,5))\n",
    "        \n",
    "    return surface_area_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the function for all of the reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_list = []\n",
    "for ID in tqdm(ID_list, total=len(ID_list)):\n",
    "    if (ID in data_dict.keys()) and (ID in results.keys()):\n",
    "        data = image_prod(ID, data_dict[ID], results[ID], make_plots = False)\n",
    "        array_list.append(data)\n",
    "        del data\n",
    "    else:\n",
    "        print('we didnt find', ID)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a look up table of depth to surface area for each gauge ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up_table = []\n",
    "for i in array_list:\n",
    "    df = DataFrame(i, columns = ['ID', 'Depth', 'Surface Area', 'number of images'])\n",
    "    look_up_table.append(df)\n",
    "\n",
    "df = pd.concat(look_up_table)\n",
    "df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the look up table as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('depth_to_surface_tables.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
